{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv) == 2 and sys.argv[1].upper() == 'YES':\n",
    "    ignore_step = 'lowercase'\n",
    "else:\n",
    "    ignore_step = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = pd.read_csv('Fake.csv')\n",
    "real_news = pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news['class'] = 0  \n",
    "real_news['class'] = 1  \n",
    "\n",
    "data = pd.concat([fake_news, real_news], ignore_index=True)\n",
    "data = data.sample(frac=1).reset_index(drop=True)  ## Shuffle\n",
    "data['text'] = data['title'] + ' ' + data['text']  ## Combine title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() #Stemmer that will be used for stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "##If argument YES given, ingore_step will be 'lowercase', and lowercasing step will be skipped.\n",
    "if ignore_step != 'lowercase':\n",
    "    data['text'] = data['text'].apply(lambda x: x.lower())  # Lowercase\n",
    "##Remove Stopwords\n",
    "    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))  # Remove stop words\n",
    "##Perfrom Stemming\n",
    "    data['text'] = data['text'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))  # Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object with binary=True and add-1 smoothing\n",
    "vectorizer = CountVectorizer(binary=True, lowercase=True, analyzer='word', min_df=1, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the documents\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "# Transform the documents into a binary bag of words\n",
    "binary_bow_matrix = vectorizer.transform(X_train).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imchriskelly',\n",
       " 'counselors',\n",
       " 'dichotom',\n",
       " 'hickory',\n",
       " 'grant',\n",
       " 'chozick',\n",
       " 'naiveti',\n",
       " 'restrepo',\n",
       " '2lnpkaq',\n",
       " 'wwba',\n",
       " 'breaks',\n",
       " 'fitzmorri',\n",
       " 'psyd',\n",
       " 'shakespeare',\n",
       " 'bernie',\n",
       " 'barometr',\n",
       " 'floundering',\n",
       " 'imbassahi',\n",
       " 'drewluminati_',\n",
       " 'moors',\n",
       " 'hubertus',\n",
       " 'clny',\n",
       " 'soot',\n",
       " 'frizzel',\n",
       " 'fever',\n",
       " 'gwadar',\n",
       " 'tunics',\n",
       " 'trudi',\n",
       " 'mineworkers',\n",
       " 'segolen',\n",
       " 'haven',\n",
       " 'iacaucu',\n",
       " 'redbox',\n",
       " 'aie0wvbokv',\n",
       " 'repos',\n",
       " 'roschdi',\n",
       " 'galaxy',\n",
       " 'breastpocket',\n",
       " 'recognizes',\n",
       " 'goali',\n",
       " 'cbn',\n",
       " 'treepublican',\n",
       " 'genis',\n",
       " 'hendrean',\n",
       " 'hierarch',\n",
       " 'reformation',\n",
       " 'liberty',\n",
       " 'o362ugxmd2',\n",
       " 'gjdtaki6o',\n",
       " 'clint',\n",
       " 'best',\n",
       " 'rajapaksa',\n",
       " 'bstandsforb',\n",
       " 'kangaroo',\n",
       " 'mad_jamaican',\n",
       " 'minu',\n",
       " 'barnett',\n",
       " 'frommer',\n",
       " 'dixit',\n",
       " 'transparently',\n",
       " 'hollingsbee',\n",
       " 'onda',\n",
       " 'wfmz',\n",
       " 'librevil',\n",
       " 'murithi',\n",
       " 'tristano',\n",
       " 'rabbis',\n",
       " 'synopses',\n",
       " 'ipera',\n",
       " 'richey',\n",
       " 'giffiords',\n",
       " 'kaoru',\n",
       " 'kvvt2tgwfd',\n",
       " 'grammy',\n",
       " 'wh2mpjvqzlgirls',\n",
       " 'lilt',\n",
       " 'simonwdc',\n",
       " 'thirti',\n",
       " 'species',\n",
       " 'techno',\n",
       " 'vindict',\n",
       " 'argo',\n",
       " 'telegraphread',\n",
       " 'kazuko',\n",
       " 'cooler',\n",
       " 'materi',\n",
       " 'ogletree',\n",
       " 'repawn',\n",
       " 'lydiahudgens',\n",
       " 'aarp',\n",
       " 'sodom',\n",
       " 'confirms',\n",
       " 'terroristtrump',\n",
       " 'puff',\n",
       " 'sheil',\n",
       " 'shoehorn',\n",
       " 'truckers',\n",
       " 'disequilibrium',\n",
       " 'shifts',\n",
       " 'trmmp',\n",
       " '2016which',\n",
       " 'gprare',\n",
       " 'zlvxzmmynm',\n",
       " 'katich',\n",
       " 'diffid',\n",
       " 'tigers',\n",
       " 'vb',\n",
       " 'soberli',\n",
       " 'tlumacki',\n",
       " 'britain',\n",
       " 'hatami',\n",
       " 'fotini',\n",
       " 'wbtv_new',\n",
       " 'lighthous',\n",
       " 'mcgee',\n",
       " 'lauding',\n",
       " 'tiong',\n",
       " 'origin',\n",
       " 'actuelles',\n",
       " 'brouillett',\n",
       " 'silos',\n",
       " 'rolex',\n",
       " 'khural',\n",
       " 'shaquil',\n",
       " 'superbowl',\n",
       " 'steink',\n",
       " 'doman',\n",
       " 'cecilia',\n",
       " 'berm',\n",
       " 'thwarted',\n",
       " 'ignoring',\n",
       " 'brewers',\n",
       " '201',\n",
       " 'behest',\n",
       " 'rapidan',\n",
       " 'yushan',\n",
       " 'nightlin',\n",
       " 'spr',\n",
       " 'nonspecif',\n",
       " 'latenightseth',\n",
       " 'yarbrough',\n",
       " '__mref',\n",
       " 'zwickau',\n",
       " '1201457439864507',\n",
       " 'coexist',\n",
       " '35pm',\n",
       " 'capac',\n",
       " 'hidstori',\n",
       " 'hillfox',\n",
       " 'tilak',\n",
       " 'icao',\n",
       " 'byong',\n",
       " 'jiji',\n",
       " 'confirmationhear',\n",
       " 'mushani',\n",
       " 'intuitive',\n",
       " 'vacations',\n",
       " 'velika',\n",
       " 'divers',\n",
       " 'plated',\n",
       " '1phhuc9',\n",
       " 'menacingli',\n",
       " 'eloquii',\n",
       " 'davaughn',\n",
       " 'preaching',\n",
       " 'estuary',\n",
       " 'logos',\n",
       " 'zawya',\n",
       " 'archaeolog',\n",
       " '2017hopefully',\n",
       " 'selma51',\n",
       " 'battlelin',\n",
       " 'comhowever',\n",
       " 'continent',\n",
       " 'loqueando',\n",
       " 'phoenixrally',\n",
       " 'deploraebl',\n",
       " '65',\n",
       " '8wbvyueh6x',\n",
       " 'benc',\n",
       " 'staehle',\n",
       " 'neurotoxin',\n",
       " 'finneburgh',\n",
       " 'faceboo',\n",
       " 'sharef',\n",
       " '2015bi',\n",
       " 'federalistof',\n",
       " 'molyneux',\n",
       " '7lhykiloyz',\n",
       " 'mckinleylynaya',\n",
       " 'romania',\n",
       " 'ruoff',\n",
       " 'zitzewitz',\n",
       " 'afzfmmsvgc',\n",
       " 'hayes',\n",
       " 'models',\n",
       " 'secretservice',\n",
       " 'abiding',\n",
       " 'mischa',\n",
       " 'rigged',\n",
       " 'thoughtless',\n",
       " 'minibus',\n",
       " 'ramiro',\n",
       " 'bermenschen',\n",
       " 'subtitle',\n",
       " 'burrough',\n",
       " 'prepar',\n",
       " 'tzotzil',\n",
       " 'angos',\n",
       " 'bilateral',\n",
       " 'crossings',\n",
       " 'nunciatur',\n",
       " 'lenard',\n",
       " 'marquardt',\n",
       " 'harrydisco',\n",
       " 'probat',\n",
       " 'yiwei',\n",
       " 'mln',\n",
       " 'mh',\n",
       " '2017torr',\n",
       " 'enzi',\n",
       " 'tombossert45',\n",
       " 'headwind',\n",
       " 'earn',\n",
       " 'yimu9tyl5n',\n",
       " 'serbiamelania',\n",
       " 'websit',\n",
       " 'ahavafeldman1',\n",
       " 'selena',\n",
       " 'kem',\n",
       " 'elat',\n",
       " 'tima',\n",
       " 'salari',\n",
       " 'f9k8svgd2h',\n",
       " 'musila',\n",
       " 'punked',\n",
       " 'reassuringli',\n",
       " 'weinbrecht',\n",
       " 'salardu',\n",
       " 'oetrl6m4a5',\n",
       " 'priceread',\n",
       " 'gushecki',\n",
       " 'gord',\n",
       " 'colinjones',\n",
       " 'soule',\n",
       " 'twa',\n",
       " '90k',\n",
       " 'insati',\n",
       " 'dorrian',\n",
       " 'discount',\n",
       " 'usgs',\n",
       " 'mikareport',\n",
       " 'committ',\n",
       " 'tell',\n",
       " 'mpd',\n",
       " '152',\n",
       " 'deviate',\n",
       " 'bisimwa',\n",
       " 'jumped',\n",
       " 'refusals',\n",
       " 'larsenjohn',\n",
       " 'cancer',\n",
       " 'lcljndpgbt',\n",
       " 'premis',\n",
       " 'jugg',\n",
       " 'saradin',\n",
       " 'generalized',\n",
       " 'connoisseur',\n",
       " 'daskam',\n",
       " 'fl1u9r6vxm',\n",
       " 'nl8n1mp0gi',\n",
       " 'stabile',\n",
       " 'haddon',\n",
       " 'mwv1queqac',\n",
       " 'ahvazi',\n",
       " 'spare',\n",
       " 'mancino',\n",
       " 'huxley',\n",
       " 'allis',\n",
       " 'smartness',\n",
       " 'halle',\n",
       " 'susan_hennessey',\n",
       " '115m',\n",
       " 'meckler',\n",
       " 'overvalu',\n",
       " 'riordan',\n",
       " 'peterle',\n",
       " 'baseload',\n",
       " 'inflamm',\n",
       " 'governorperri',\n",
       " 'hypocritically',\n",
       " 'eona6yett2',\n",
       " 'weeks',\n",
       " 'badiandsnps',\n",
       " 'mosby',\n",
       " 'gramm',\n",
       " 'meaningful',\n",
       " '34am',\n",
       " 'equivocated',\n",
       " 'rafael',\n",
       " 'unrelated',\n",
       " 'twittercuomo',\n",
       " 'unconfirmed',\n",
       " 'kimjongilia',\n",
       " 'bereaved',\n",
       " 'vsu',\n",
       " 'zavalla',\n",
       " 'attiko',\n",
       " 'blemishes',\n",
       " 'ckbag',\n",
       " 'fr37kw8miv',\n",
       " 'equipment',\n",
       " 'neonazi',\n",
       " 'unchivalrous',\n",
       " 'nl1n19301n',\n",
       " 'rotunda',\n",
       " 'sjxjavssrr',\n",
       " 'reuniting',\n",
       " 'khmaclean',\n",
       " 'transform',\n",
       " 'understand',\n",
       " 'rambla',\n",
       " 'stonehil',\n",
       " 'tumbeni',\n",
       " '1981',\n",
       " 'charvieu',\n",
       " 'ame',\n",
       " 'orifices',\n",
       " 'scissors',\n",
       " 'tech30',\n",
       " 'bal',\n",
       " '865',\n",
       " 'sarah4justic',\n",
       " 'undervalued',\n",
       " 'ashara',\n",
       " 'sakes',\n",
       " 'lexsage',\n",
       " 'leonidio',\n",
       " 'ardor',\n",
       " 'unsocial',\n",
       " 'inquest',\n",
       " 'mozambique',\n",
       " 'clutches',\n",
       " '57am',\n",
       " 'adp',\n",
       " 'danang',\n",
       " 'kakistocracy',\n",
       " 'shimon',\n",
       " '200th',\n",
       " 'offtak',\n",
       " 'brig',\n",
       " 'kristi',\n",
       " 'songwriter',\n",
       " 'outreach',\n",
       " 'puns',\n",
       " 'steinbrugge',\n",
       " 'holt',\n",
       " 'putinthefinishingtouch',\n",
       " 'scape',\n",
       " 'creeds',\n",
       " 'pipersul',\n",
       " 'yrs',\n",
       " 'nearby',\n",
       " 'viennathes',\n",
       " 'lazard',\n",
       " 'tremain',\n",
       " 'superbodi',\n",
       " 'porridg',\n",
       " 'carandang',\n",
       " 'radicalized',\n",
       " 'masoud',\n",
       " 'coalinga',\n",
       " 'stray',\n",
       " 'bookkeeper',\n",
       " 'aost',\n",
       " 'saul',\n",
       " 'icm',\n",
       " 'table',\n",
       " 'hectormorenco',\n",
       " 'homogeneous',\n",
       " 'optimist',\n",
       " 'tabernacle',\n",
       " 'browski',\n",
       " 'vecernj',\n",
       " 'kinzel',\n",
       " 'cakewalk',\n",
       " 'dhoud',\n",
       " 'miftah',\n",
       " 'crenshaw',\n",
       " '1984a',\n",
       " 'reconfiguring',\n",
       " 'sensuality',\n",
       " 'shillbeta',\n",
       " 'sistan',\n",
       " 'miraz',\n",
       " 'tulan',\n",
       " 'etfs',\n",
       " 'leylek',\n",
       " 'holster',\n",
       " 'abdelazim',\n",
       " 'depor',\n",
       " 'thereof',\n",
       " 'rufin',\n",
       " 'whisenant',\n",
       " 'whitspen',\n",
       " 'maryemilyohara',\n",
       " 'newswouldn',\n",
       " 'lassitud',\n",
       " 'pilchuck',\n",
       " 'decoded',\n",
       " 'ten_gopbreaking',\n",
       " 'eloise',\n",
       " 'yeatesgreats',\n",
       " 'scrambling',\n",
       " 'mato',\n",
       " 'bse',\n",
       " 'dwstweet',\n",
       " 'jpg',\n",
       " 'cursiv',\n",
       " 'karnei',\n",
       " 'despot',\n",
       " 'kickbacks',\n",
       " 'dad',\n",
       " 'reservists',\n",
       " 'citibik',\n",
       " 'sewol',\n",
       " 'tamils',\n",
       " 'delahanty',\n",
       " 'efta',\n",
       " 'righeim',\n",
       " 'kankake',\n",
       " 'afw1vjwvwh',\n",
       " 'cheema',\n",
       " 'scanners',\n",
       " 'kerlikowske',\n",
       " 'femin',\n",
       " 'traitorous',\n",
       " 'will_the_tiger',\n",
       " 'screeengrab',\n",
       " 'youshouldmaryme',\n",
       " 'jaketurx',\n",
       " 'charli',\n",
       " 'joeym728',\n",
       " 'handel',\n",
       " 'shatn',\n",
       " 'buckl',\n",
       " 'enar',\n",
       " 'kills',\n",
       " 'lapan',\n",
       " 'spiel',\n",
       " 'eulog',\n",
       " 'storag',\n",
       " 'vpdlhffpb8',\n",
       " 'vvtrrekmgj',\n",
       " 'unfeasible',\n",
       " 'rogersandtaylor',\n",
       " 'cancelation',\n",
       " 'indirectly',\n",
       " '776879408724541444',\n",
       " 'mineral',\n",
       " 'thorni',\n",
       " 'prashant',\n",
       " 'warmbier',\n",
       " 'ubipages',\n",
       " 'ermey',\n",
       " 'fuels',\n",
       " 'performances',\n",
       " 'flaring',\n",
       " 'manila',\n",
       " '62the',\n",
       " 'gouda',\n",
       " 'yemeni',\n",
       " 'antoinette',\n",
       " 'picardo',\n",
       " 'theunsilentmajor',\n",
       " 'boubacar',\n",
       " 'woolen',\n",
       " 'gomes',\n",
       " 'chandrakirana',\n",
       " 'hwkyfxw7uw',\n",
       " 'glasgow',\n",
       " 'guinan',\n",
       " 'pollock',\n",
       " 'takeyh',\n",
       " 'belittle',\n",
       " 'normalize',\n",
       " 'jpyz4bkiok',\n",
       " 'behr',\n",
       " 'victimless',\n",
       " 'gourguechon',\n",
       " 'youontheoutsidelookingin',\n",
       " 'legible',\n",
       " 'juden',\n",
       " 'trilog',\n",
       " 'indrajono',\n",
       " 'r8dgsibelnqher',\n",
       " 'engagement',\n",
       " 'businessman',\n",
       " 'hezekiah',\n",
       " 'qmd7xe0cjc',\n",
       " 'hagl',\n",
       " 'cri',\n",
       " 'incubu',\n",
       " 'huresist',\n",
       " 'mailwhit',\n",
       " 'smsing',\n",
       " 'surfkitti',\n",
       " '2018lindsey',\n",
       " '833',\n",
       " '8n_xvla_5loth',\n",
       " 'statia',\n",
       " 'jess',\n",
       " 'ndrew_lawrence',\n",
       " 'banshee',\n",
       " '3422',\n",
       " 'ewing',\n",
       " 'speweth',\n",
       " 'shermin',\n",
       " '2016don',\n",
       " 'barcod',\n",
       " 'infants',\n",
       " 'chizewer',\n",
       " 'infield',\n",
       " 'highfalutin',\n",
       " '1154',\n",
       " 'nonconsecut',\n",
       " 'steadfast',\n",
       " 'statski',\n",
       " '2017kick',\n",
       " 'entendre',\n",
       " 'maddin',\n",
       " 'timesa',\n",
       " 'newsnick',\n",
       " 'sunnyjl52',\n",
       " 'zuleta',\n",
       " 'keable',\n",
       " 'presumed',\n",
       " 'turbotax',\n",
       " 'vitale',\n",
       " 'vibrio',\n",
       " 'zerodarkthirti',\n",
       " 'launchers',\n",
       " 'electorates',\n",
       " 'puente',\n",
       " 'treehouseher',\n",
       " 'secretaryofstate',\n",
       " 'mismari',\n",
       " 'curtaton',\n",
       " 'gindot12',\n",
       " 'suntrust',\n",
       " 'dalio',\n",
       " 'goldenboy',\n",
       " 'yarjani',\n",
       " 'rosenbach',\n",
       " 'wermiel',\n",
       " 'tefft',\n",
       " 'defrees',\n",
       " 'sadani',\n",
       " 'trapped',\n",
       " 'fortieth',\n",
       " 'axipaamc2',\n",
       " '7dlpjc2dwi',\n",
       " 'orchestra',\n",
       " 'russiaintelhear',\n",
       " 'pilat',\n",
       " 'rooted',\n",
       " 'mdxghx2goo',\n",
       " 'keyanna',\n",
       " 'vc_column',\n",
       " 'siluanov',\n",
       " 'withold',\n",
       " 'immedi',\n",
       " 'unthink',\n",
       " 'nvzxpfg3j0',\n",
       " 'melaniatrump',\n",
       " 'unrecognizable',\n",
       " 'tcxp2dlpa0',\n",
       " 'daisy',\n",
       " '2016full',\n",
       " 'northamptonshire',\n",
       " 'subtlety',\n",
       " 'pm',\n",
       " 'tuvalu',\n",
       " 'weapons',\n",
       " 'cnnleak',\n",
       " 'inerti',\n",
       " 'ionian',\n",
       " '10pm',\n",
       " 'ghostbust',\n",
       " 'jazz',\n",
       " 'ke',\n",
       " 'mockabl',\n",
       " 'maildur',\n",
       " 'delbrocco',\n",
       " 'onaodowan',\n",
       " 'dayjustin',\n",
       " 'tm',\n",
       " 'yalta',\n",
       " 'evading',\n",
       " '0c1jiaqxwn',\n",
       " 'inconsolable',\n",
       " 'kloeckner',\n",
       " 'celic',\n",
       " '846',\n",
       " 'admonish',\n",
       " 'dispatch',\n",
       " 'reprimanded',\n",
       " 'jumpsuit',\n",
       " 'jefferycupcakes',\n",
       " 'traips',\n",
       " 'okiedokiepokey',\n",
       " 'camil',\n",
       " '250kg',\n",
       " 'guevarra',\n",
       " 'unconcern',\n",
       " 'dress',\n",
       " 'georgini',\n",
       " 'nonleth',\n",
       " 'kawangwar',\n",
       " 'litig',\n",
       " 'trumpsters',\n",
       " 'vdma',\n",
       " 'breaking911',\n",
       " 'fanatic',\n",
       " 'summari',\n",
       " 'abadi',\n",
       " 'gallow',\n",
       " 'misdeal',\n",
       " 'prezadint',\n",
       " 'seldom',\n",
       " 'todayov',\n",
       " '2016furthermore',\n",
       " 'civey',\n",
       " 'stab',\n",
       " 'savonlinna',\n",
       " '2017no',\n",
       " 'niquett',\n",
       " 'gist',\n",
       " 'spideroak',\n",
       " 'techniqu',\n",
       " 'l2n1n02uy',\n",
       " 'longitud',\n",
       " 'dwindled',\n",
       " 'yiannopoulos',\n",
       " 'peg',\n",
       " 'kataeb',\n",
       " 'alohakatie',\n",
       " 'shaykh',\n",
       " 'hensarl',\n",
       " 'statur',\n",
       " 'arak',\n",
       " 'reservoir',\n",
       " 'forehead',\n",
       " 'tractors',\n",
       " 'fincinc',\n",
       " 'jeremycorbyn',\n",
       " 'candy',\n",
       " 'lioness',\n",
       " 'zmji36q8e4ovia',\n",
       " 'videocast',\n",
       " 'evancho',\n",
       " 'tribunals',\n",
       " 'brat',\n",
       " 'germanista',\n",
       " 'analys',\n",
       " 'mrny',\n",
       " 'baden',\n",
       " 'hoitenga',\n",
       " 'jxpgcdedgf',\n",
       " 'rigsbys',\n",
       " 'counts',\n",
       " 'conakry',\n",
       " 'clem',\n",
       " 'destabilize',\n",
       " 'kiting',\n",
       " 'leguizamon',\n",
       " 'brothel',\n",
       " 'coordinates',\n",
       " 'reconfirm',\n",
       " 'canpoli',\n",
       " 'juussstttt',\n",
       " 'burwel',\n",
       " 'hotbeansmorgan',\n",
       " 'jug',\n",
       " 'j6rc9l5520',\n",
       " '7uksbhp7xq',\n",
       " 'keener',\n",
       " 'whit',\n",
       " 'miteb',\n",
       " 'sacrilegi',\n",
       " 'bermuda',\n",
       " 'gasket',\n",
       " 'gazprombank',\n",
       " '2017chaffetz',\n",
       " 'alcantara',\n",
       " '1814',\n",
       " 'frogmen',\n",
       " 'gracehelbig',\n",
       " 'zx7gkfbuv4',\n",
       " 'kalenjin',\n",
       " 'uninsured',\n",
       " 'leth',\n",
       " 'ecowa',\n",
       " 'racist',\n",
       " 'probationthi',\n",
       " 'hellman',\n",
       " 'sensationalists',\n",
       " 'sitcoms',\n",
       " 'hushed',\n",
       " '467',\n",
       " 'doshka',\n",
       " 'obegefel',\n",
       " '8new',\n",
       " 'mayweath',\n",
       " 'westiran',\n",
       " 'ndlevinson',\n",
       " 'bustl',\n",
       " 'mrozek',\n",
       " 'gradient',\n",
       " 'heartach',\n",
       " 'subversive',\n",
       " 'prepaid',\n",
       " 'hoon',\n",
       " 'hookers',\n",
       " 'keratsini',\n",
       " 'microscopeov',\n",
       " 'autopilot',\n",
       " 'documentari',\n",
       " 'dinnerth',\n",
       " 'fall',\n",
       " 'artisanally',\n",
       " 'haslett',\n",
       " 'uncrit',\n",
       " 'simonmaloy',\n",
       " 'ratas',\n",
       " 'ibderi',\n",
       " 'kendra',\n",
       " 'abound',\n",
       " 'rank',\n",
       " 'overcrowded',\n",
       " 'nicknames',\n",
       " 'proxi',\n",
       " 'cause',\n",
       " 'drbencarson',\n",
       " 'bushwood',\n",
       " 'charlespm777',\n",
       " 'ailments',\n",
       " 'glu',\n",
       " 'napolitan',\n",
       " 'swerdlow',\n",
       " 'spindoctor',\n",
       " 'cartoonishli',\n",
       " 'staples',\n",
       " 'steps',\n",
       " 'upstat',\n",
       " 'asdf',\n",
       " 'chisinau',\n",
       " '2017consid',\n",
       " 'misstatements',\n",
       " 'racialist',\n",
       " 'schrader',\n",
       " 'deadstatetweets',\n",
       " 'midwestrico',\n",
       " 'fifth',\n",
       " 'whiski',\n",
       " 'ibsceswl9f',\n",
       " 'pinwheel',\n",
       " 'glf9onjx7g',\n",
       " 'havel',\n",
       " 'pawelski',\n",
       " '059',\n",
       " 'sovereigns',\n",
       " 'hilli',\n",
       " 'nl1n1ol160',\n",
       " 'sanctimonious',\n",
       " 'stevensantos',\n",
       " 'cramping',\n",
       " 'scared',\n",
       " '25142',\n",
       " 'defensible',\n",
       " 'dismissed',\n",
       " 'morrow',\n",
       " 'unisono',\n",
       " 'meti',\n",
       " 'savitz',\n",
       " 'jjauthor',\n",
       " 'fishel',\n",
       " 'doyoujob',\n",
       " 'eleventi',\n",
       " 'drink',\n",
       " 'zuccotti',\n",
       " 'juliana',\n",
       " 'fifties',\n",
       " 'patients',\n",
       " 'aerospac',\n",
       " 'barbitur',\n",
       " 'gopmovi',\n",
       " 'alvpdlym85',\n",
       " 'culhan',\n",
       " 'noppen',\n",
       " '0746',\n",
       " 'fightersfrom',\n",
       " 'drunks',\n",
       " 'coalit',\n",
       " 'uci',\n",
       " 'jaaezldosq',\n",
       " 'ennio',\n",
       " 'mwpolitics',\n",
       " 'anaida',\n",
       " 'indebt',\n",
       " 'blackwat',\n",
       " '789077954556576',\n",
       " 'perry',\n",
       " 'tossapon',\n",
       " 'functioningilliterates',\n",
       " 'ax7gxzlwzj',\n",
       " 'chahed',\n",
       " 'etxqmvlqkd',\n",
       " 'killeen',\n",
       " 'bafilo',\n",
       " 'fixman',\n",
       " 'maktab',\n",
       " 'newswire',\n",
       " '7vhuqpejc0',\n",
       " 'respectourflag',\n",
       " '2qydwwy',\n",
       " 'celebrate',\n",
       " 'dallek',\n",
       " 'curtain',\n",
       " 'conservativesgroup',\n",
       " 'pyn92mesom',\n",
       " 'slacktivists',\n",
       " 'd91eyacimz',\n",
       " 'clintonhillari',\n",
       " 'illitches',\n",
       " 'inclusively',\n",
       " 'birthdat',\n",
       " 'johnjohnsonson',\n",
       " 'simcer70swon',\n",
       " 'unmasking',\n",
       " '2cz8iu1',\n",
       " 'fireeye',\n",
       " 'nakiqkn9baea',\n",
       " 'transient',\n",
       " 'gergel',\n",
       " 'nchinda',\n",
       " 'evening',\n",
       " 'c40',\n",
       " 'kippur',\n",
       " 'launches',\n",
       " 'mustache',\n",
       " 'popculturelol16',\n",
       " 'advise',\n",
       " 'loyd',\n",
       " '2017visitor',\n",
       " '7aicubjlud',\n",
       " 'leami',\n",
       " 'stoupa',\n",
       " 'alsobecaus',\n",
       " 'despotism',\n",
       " 'hbcu',\n",
       " 'jmiyrl9ofns',\n",
       " 'broadcasters',\n",
       " 'mckenzie',\n",
       " 'welders',\n",
       " 'acquaint',\n",
       " 'horrificli',\n",
       " 'whoopi',\n",
       " 'isan',\n",
       " 'itc',\n",
       " 'hebert',\n",
       " 'keith',\n",
       " 'unravelling',\n",
       " 'mfaku',\n",
       " 'razor',\n",
       " 'leggatt',\n",
       " 'rem',\n",
       " 'egyptair',\n",
       " 'stephanopouloseveri',\n",
       " 'disposed',\n",
       " 'indianapolis2',\n",
       " 'bites',\n",
       " 'fkn',\n",
       " 'varianc',\n",
       " 'exeter',\n",
       " 'attentiveness',\n",
       " 'babel',\n",
       " 'rxoohjmzmew',\n",
       " 'nl8n1nr57c',\n",
       " 'argumentation',\n",
       " 'birthers',\n",
       " 'levantin',\n",
       " 'a31zfkm3rx',\n",
       " 'rtfbg',\n",
       " 'upstag',\n",
       " 'capit',\n",
       " 'additions',\n",
       " 'woke',\n",
       " 'elegantli',\n",
       " 'elitesaccord',\n",
       " 'destabil',\n",
       " 'motherland',\n",
       " 'sevareid',\n",
       " 'untaint',\n",
       " 'rncinclehttps',\n",
       " 'anomalies',\n",
       " 'marro',\n",
       " 'nxexjsab64',\n",
       " 'blistering',\n",
       " 'connah',\n",
       " 'sana',\n",
       " 'vjboedynuw',\n",
       " 'loops',\n",
       " 'prefigur',\n",
       " 'russellrelliott',\n",
       " 'clayartist2',\n",
       " 'wk',\n",
       " 'krueger',\n",
       " 'crickets',\n",
       " 'bllion',\n",
       " 'pontiac',\n",
       " 'melco',\n",
       " 'birra',\n",
       " 'banquets',\n",
       " 'allen1006',\n",
       " 'heighten',\n",
       " 'ramkissoon',\n",
       " 'sounds',\n",
       " 'affirm',\n",
       " 'ujye100tn9',\n",
       " 'pidgin',\n",
       " 'jeopardi',\n",
       " 'coordinating',\n",
       " 'ubucky',\n",
       " 'burdzi',\n",
       " 'gsanroman2',\n",
       " 'curtsi',\n",
       " 'mprnew',\n",
       " 'yucatan',\n",
       " 'carrier',\n",
       " 'rasmussen',\n",
       " 'doubling',\n",
       " 'eighteen',\n",
       " 'balaya',\n",
       " 'overgraz',\n",
       " 'condril',\n",
       " '6326',\n",
       " 'catbird',\n",
       " 'demconvention',\n",
       " 'glowed',\n",
       " 'huppke',\n",
       " '2016elect',\n",
       " 'sy',\n",
       " 'tepco',\n",
       " 'wussifi',\n",
       " 'haggl',\n",
       " 'harlan',\n",
       " 'chilly',\n",
       " 'bekir',\n",
       " 'underlin',\n",
       " 'mumin',\n",
       " 'ltv',\n",
       " 'existence',\n",
       " 'jjghveb63',\n",
       " 'aversion',\n",
       " 'luhrman',\n",
       " 'smaland',\n",
       " 'speedier',\n",
       " 'bidzina',\n",
       " 'galmudug',\n",
       " 'ferrera',\n",
       " 'deplorability',\n",
       " 'rand',\n",
       " 'monoton',\n",
       " 'striker',\n",
       " 'dortmund',\n",
       " 'kwok',\n",
       " 'normoyl',\n",
       " 'collide',\n",
       " 'saga',\n",
       " 'rastriya',\n",
       " 'corettascottk',\n",
       " '120k',\n",
       " 'ilenc',\n",
       " 'radars',\n",
       " 'sukhoi',\n",
       " 'strenuou',\n",
       " 'makers',\n",
       " 'jaywillis',\n",
       " 'vpwyv7lc3fa',\n",
       " '315',\n",
       " 'l8n1nk5ek',\n",
       " '985',\n",
       " 'congenital',\n",
       " 'simonds',\n",
       " 'atienza',\n",
       " 'alisonleiby',\n",
       " 'airstrik',\n",
       " 'inv',\n",
       " 'ymtr3trgj8',\n",
       " ...}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set(vectorizer.get_feature_names_out())\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, fake_word_counts={}, real_word_counts={}, fake_docs_count=0, real_docs_count=0):\n",
    "        self.fake_word_counts = fake_word_counts\n",
    "        self.real_word_counts = real_word_counts\n",
    "        self.fake_docs_count = fake_docs_count\n",
    "        self.real_docs_count = real_docs_count\n",
    "\n",
    "        # Add dictionary attributes to keep track of probabilities\n",
    "        self.fake_probs = {}\n",
    "        self.real_probs = {}\n",
    "        self.p_fake = None\n",
    "        self.p_real = None\n",
    "    \n",
    "    \n",
    "    def train(self, X, y):\n",
    "        # Split the bag of words matrix into two matrices, one for spam and one for ham\n",
    "        X_fake = X[y == 1]\n",
    "        X_real = X[y == 0]\n",
    "\n",
    "        # Calculate the prior probabilities for each class\n",
    "        self.p_fake = len(X_fake) / len(X)\n",
    "        self.p_real = len(X_real) / len(X)\n",
    "\n",
    "        # Calculate the likelihood probabilities for each word\n",
    "        vocabulary = set(list(self.fake_word_counts.keys()) + list(self.real_word_counts.keys()))\n",
    "        fake_word_counts = np.array([self.fake_word_counts.get(word, 0) for word in vocabulary])\n",
    "        real_word_counts = np.array([self.real_word_counts.get(word, 0) for word in vocabulary])\n",
    "        self.fake_probs = (fake_word_counts + 1) / (np.sum(X_fake) + len(vocabulary))\n",
    "        self.real_probs = (real_word_counts + 1) / (np.sum(X_real) + len(vocabulary))\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for i in range(len(X)):\n",
    "            # Calculate the log probabilities of the document belonging to each class\n",
    "            fake_log_prob = np.log(self.p_fake)\n",
    "            real_log_prob = np.log(self.p_real)\n",
    "            for j in range(len(X[i])):\n",
    "                if X[i][j] > 0:\n",
    "                    fake_log_prob += X[i][j] * np.log(self.fake_probs.get(j, 1))\n",
    "                    real_log_prob += X[i][j] * np.log(self.real_probs.get(j, 1))\n",
    "            # Choose the class with the higher probability\n",
    "            if fake_log_prob > real_log_prob:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "vocabulary = set()\n",
    "fake_word_counts = {}\n",
    "real_word_counts = {}\n",
    "fake_docs_count = 0\n",
    "real_docs_count = 0\n",
    "for i in range(len(X_train)):\n",
    "    words = set(X_train.iloc[i].split())\n",
    "    vocabulary = vocabulary.union(words)\n",
    "    if y_train.iloc[i] == 0:\n",
    "        fake_docs_count += 1\n",
    "        for word in words:\n",
    "            if word not in fake_word_counts:\n",
    "                fake_word_counts[word] = 1\n",
    "            else:\n",
    "                fake_word_counts[word] += 1\n",
    "    else:\n",
    "        real_docs_count += 1\n",
    "        for word in words:\n",
    "            if word not in real_word_counts:\n",
    "                real_word_counts[word] = 1\n",
    "            else:\n",
    "                real_word_counts[word] += 1\n",
    "\n",
    "fake_prior_prob = fake_docs_count / len(X_train)\n",
    "real_prior_prob = real_docs_count / len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = NaiveBayesClassifier(fake_word_counts, real_word_counts, fake_docs_count, real_docs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.train(binary_bow_matrix,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(document, fake_word_counts, real_word_counts,fake_prior_prob, real_prior_prob, vocabulary):\n",
    "    words = set(document.split())\n",
    "    fake_prob = 0.0 \n",
    "    real_prob = 0.0\n",
    "    ## Apply Bayes Rule with lapace 1 smoothing\n",
    "    for word in vocabulary:\n",
    "        if word in fake_word_counts: \n",
    "            fake_prob += log((fake_word_counts[word] + 1) /\n",
    "                (sum(fake_word_counts.values()) + len(vocabulary))) \n",
    "        \n",
    "        else:\n",
    "            fake_prob += log(1 / (sum(fake_word_counts.values()) + len(vocabulary)))\n",
    "        if word in real_word_counts:\n",
    "            real_prob += log((real_word_counts[word] + 1) / (sum(real_word_counts.values()) + len(vocabulary)))\n",
    "        else:\n",
    "            real_prob += log(1 / (sum(real_word_counts.values()) + len(vocabulary)))\n",
    "        \n",
    "    fake_prob += log(fake_prior_prob)\n",
    "    real_prob += log(real_prior_prob)\n",
    "\n",
    "    if fake_prob > real_prob:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_test)):\n\u001b[1;32m      4\u001b[0m     document \u001b[39m=\u001b[39m X_test\u001b[39m.\u001b[39miloc[i]\n\u001b[0;32m----> 5\u001b[0m     predicted_class \u001b[39m=\u001b[39m classify(document, fake_word_counts, real_word_counts, fake_prior_prob, real_prior_prob, vocabulary)\n\u001b[1;32m      6\u001b[0m     true_class \u001b[39m=\u001b[39m y_test\u001b[39m.\u001b[39miloc[i]\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Calculate the counts of true positives, true negatives, false positives, and false negatives\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mclassify\u001b[0;34m(document, fake_word_counts, real_word_counts, fake_prior_prob, real_prior_prob, vocabulary)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m vocabulary:\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m fake_word_counts:\n\u001b[1;32m      7\u001b[0m         fake_prob \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m log((fake_word_counts[word] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\n\u001b[0;32m----> 8\u001b[0m             (\u001b[39msum\u001b[39;49m(fake_word_counts\u001b[39m.\u001b[39;49mvalues()) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(vocabulary)))\n\u001b[1;32m      9\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         fake_prob \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m log(\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39msum\u001b[39m(fake_word_counts\u001b[39m.\u001b[39mvalues()) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(vocabulary)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test the classifier\n",
    "tp = tn = fp = fn = 0\n",
    "for i in range(len(X_test)):\n",
    "    document = X_test.iloc[i]\n",
    "    predicted_class = classify(document, fake_word_counts, real_word_counts, fake_prior_prob, real_prior_prob, vocabulary)\n",
    "    true_class = y_test.iloc[i]\n",
    "    \n",
    "    # Calculate the counts of true positives, true negatives, false positives, and false negatives\n",
    "    if true_class == 0 and predicted_class == 0:\n",
    "        tn += 1\n",
    "    elif true_class == 0 and predicted_class == 1:\n",
    "        fp += 1\n",
    "    elif true_class == 1 and predicted_class == 0:\n",
    "        fn += 1\n",
    "    elif true_class == 1 and predicted_class == 1:\n",
    "        tp += 1\n",
    "\n",
    "# Calculate the various metrics\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "precision = tp / (tp + fp)\n",
    "negative_predictive_value = tn / (tn + fn)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "f_score = 2 * precision * sensitivity / (precision + sensitivity)\n",
    "\n",
    "# Display the results\n",
    "print(\"Number of true positives: \", tp)\n",
    "print(\"Number of true negatives: \", tn)\n",
    "print(\"Number of false positives: \", fp)\n",
    "print(\"Number of false negatives: \", fn)\n",
    "print(\"Sensitivity: \", sensitivity)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Negative predictive value: \", negative_predictive_value)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F-score: \", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
