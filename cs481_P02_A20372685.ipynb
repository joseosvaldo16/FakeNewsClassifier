{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv) == 2 and sys.argv[1].upper() == 'YES':\n",
    "    ignore_step = 'lowercase'\n",
    "else:\n",
    "    ignore_step = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = pd.read_csv('Fake.csv')\n",
    "real_news = pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news['class'] = 0  \n",
    "real_news['class'] = 1  \n",
    "\n",
    "data = pd.concat([fake_news, real_news], ignore_index=True)\n",
    "data = data.sample(frac=1).reset_index(drop=True)  ## Shuffle\n",
    "data['text'] = data['title'] + ' ' + data['text']  ## Combine title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() #Stemmer that will be used for stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "##If argument YES given, ingore_step will be 'lowercase', and lowercasing step will be skipped.\n",
    "if ignore_step != 'lowercase':\n",
    "    data['text'] = data['text'].apply(lambda x: x.lower())  # Lowercase\n",
    "##Remove Stopwords\n",
    "    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))  # Remove stop words\n",
    "##Perfrom Stemming\n",
    "    data['text'] = data['text'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))  # Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "vocabulary = set()\n",
    "fake_word_counts = {}\n",
    "real_word_counts = {}\n",
    "fake_docs_count = 0\n",
    "real_docs_count = 0\n",
    "for i in range(len(X_train)):\n",
    "    words = set(X_train.iloc[i].split())\n",
    "    vocabulary = vocabulary.union(words)\n",
    "    if y_train.iloc[i] == 0:\n",
    "        fake_docs_count += 1\n",
    "        for word in words:\n",
    "            if word not in fake_word_counts:\n",
    "                fake_word_counts[word] = 1\n",
    "            else:\n",
    "                fake_word_counts[word] += 1\n",
    "    else:\n",
    "        real_docs_count += 1\n",
    "        for word in words:\n",
    "            if word not in real_word_counts:\n",
    "                real_word_counts[word] = 1\n",
    "            else:\n",
    "                real_word_counts[word] += 1\n",
    "\n",
    "fake_prior_prob = fake_docs_count / len(X_train)\n",
    "real_prior_prob = real_docs_count / len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(document, fake_word_counts, real_word_counts,fake_prior_prob, real_prior_prob, vocabulary):\n",
    "    words = set(document.split())\n",
    "    fake_prob = 0.0 \n",
    "    real_prob = 0.0\n",
    "    for word in vocabulary:\n",
    "        if word in fake_word_counts:\n",
    "            fake_prob += log((fake_word_counts[word] + 1) /\n",
    "                (sum(fake_word_counts.values()) + len(vocabulary)))\n",
    "        else:\n",
    "            fake_prob += log(1 / (sum(fake_word_counts.values()) + len(vocabulary)))\n",
    "        if word in real_word_counts:\n",
    "            real_prob += log((real_word_counts[word] + 1) / (sum(real_word_counts.values()) + len(vocabulary)))\n",
    "        else:\n",
    "            real_prob += log(1 / (sum(real_word_counts.values()) + len(vocabulary)))\n",
    "        \n",
    "    fake_prob += log(fake_prior_prob)\n",
    "    real_prob += log(real_prior_prob)\n",
    "\n",
    "    if fake_prob > real_prob:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_parallel(X_test.iloc[0],fake_word_counts, real_word_counts, fake_prior_prob, real_prior_prob, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_test)):\n\u001b[1;32m      4\u001b[0m     document \u001b[39m=\u001b[39m X_test\u001b[39m.\u001b[39miloc[i]\n\u001b[0;32m----> 5\u001b[0m     predicted_class \u001b[39m=\u001b[39m classify(document, fake_word_counts, real_word_counts, fake_prior_prob, real_prior_prob, vocabulary)\n\u001b[1;32m      6\u001b[0m     true_class \u001b[39m=\u001b[39m y_test\u001b[39m.\u001b[39miloc[i]\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Calculate the counts of true positives, true negatives, false positives, and false negatives\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mclassify\u001b[0;34m(document, fake_word_counts, real_word_counts, fake_prior_prob, real_prior_prob, vocabulary)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m vocabulary:\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m fake_word_counts:\n\u001b[1;32m      7\u001b[0m         fake_prob \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m log((fake_word_counts[word] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\n\u001b[0;32m----> 8\u001b[0m             (\u001b[39msum\u001b[39;49m(fake_word_counts\u001b[39m.\u001b[39;49mvalues()) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(vocabulary)))\n\u001b[1;32m      9\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         fake_prob \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m log(\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39msum\u001b[39m(fake_word_counts\u001b[39m.\u001b[39mvalues()) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(vocabulary)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test the classifier\n",
    "tp = tn = fp = fn = 0\n",
    "for i in range(len(X_test)):\n",
    "    document = X_test.iloc[i]\n",
    "    predicted_class = classify(document, fake_word_counts, real_word_counts, fake_prior_prob, real_prior_prob, vocabulary)\n",
    "    true_class = y_test.iloc[i]\n",
    "    \n",
    "    # Calculate the counts of true positives, true negatives, false positives, and false negatives\n",
    "    if true_class == 0 and predicted_class == 0:\n",
    "        tn += 1\n",
    "    elif true_class == 0 and predicted_class == 1:\n",
    "        fp += 1\n",
    "    elif true_class == 1 and predicted_class == 0:\n",
    "        fn += 1\n",
    "    elif true_class == 1 and predicted_class == 1:\n",
    "        tp += 1\n",
    "\n",
    "# Calculate the various metrics\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "precision = tp / (tp + fp)\n",
    "negative_predictive_value = tn / (tn + fn)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "f_score = 2 * precision * sensitivity / (precision + sensitivity)\n",
    "\n",
    "# Display the results\n",
    "print(\"Number of true positives: \", tp)\n",
    "print(\"Number of true negatives: \", tn)\n",
    "print(\"Number of false positives: \", fp)\n",
    "print(\"Number of false negatives: \", fn)\n",
    "print(\"Sensitivity: \", sensitivity)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Negative predictive value: \", negative_predictive_value)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F-score: \", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
